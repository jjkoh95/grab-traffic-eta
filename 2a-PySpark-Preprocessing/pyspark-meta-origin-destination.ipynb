{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import ArrayType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config setups\n",
    "conf = SparkConf()  # create the configuration\n",
    "\n",
    "bucket = \"hackathon.jjkoh.com\"\n",
    "conf.set('temporaryGcsBucket', bucket)\n",
    "conf.set('spark.app.name', 'spark-grab-challenge')\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc).builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info(df, line_to_show=5):\n",
    "    if df is not None:\n",
    "        if isinstance(df, dict):\n",
    "            for key in df:\n",
    "                print(key)\n",
    "                df[key].printSchema()\n",
    "                if line_to_show > 0:\n",
    "                    df[key].show(line_to_show)\n",
    "        else:\n",
    "            df.printSchema()\n",
    "            if line_to_show > 0:\n",
    "                df.show(line_to_show)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from BigQuery.\n",
    "df = spark.read.format('bigquery') \\\n",
    "  .option('table', 'jjkoh95:jjkoh.grab_raw_singapore') \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter 78371, 77844, 65738, 72549 (Outlier)\n",
    "df = df.filter(~df['trj_id'].isin([78371, 77844, 65738, 72549]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data and filter\n",
    "df = df.sort('trj_id', 'pingtimestamp')\n",
    "\n",
    "df = df.withColumn(\"idx\", monotonically_increasing_id())\n",
    "windowSpec = Window.partitionBy('trj_id').orderBy(\"idx\")\n",
    "df = df.withColumn(\"idx\", row_number().over(windowSpec))\n",
    "df = df.withColumn(\"remainder\", col('idx') % 60)\n",
    "df = df.filter(df.remainder == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_info(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data and filter\n",
    "df = df.sort('trj_id', 'pingtimestamp')\n",
    "\n",
    "# New column with count number\n",
    "windowSpec = Window.partitionBy('trj_id')\n",
    "duplicate_df = df.withColumn('explode', count('trj_id').over(windowSpec))\n",
    "\n",
    "# Make the count number as an array with the number of count number\n",
    "n_to_array = udf(lambda n : [n] * n, ArrayType(IntegerType()))\n",
    "duplicate_df = duplicate_df.withColumn('explode', n_to_array(duplicate_df.explode))\n",
    "\n",
    "# Explore array and delete the explode column\n",
    "duplicate_df = (duplicate_df\n",
    "                .withColumn('explode', explode(duplicate_df.explode))\n",
    "                .drop('explode')\n",
    "               )\n",
    "\n",
    "# Add index\n",
    "duplicate_df = duplicate_df.withColumn(\"idx\", monotonically_increasing_id())\n",
    "windowSpec = Window.partitionBy('trj_id').orderBy(\"idx\")\n",
    "duplicate_df = duplicate_df.withColumn(\"idx\", row_number().over(windowSpec))\n",
    "\n",
    "# print_info(duplicate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder table\n",
    "duplicate_df2 = duplicate_df.withColumn('idx', monotonically_increasing_id())\n",
    "windowSpec = Window.partitionBy('trj_id', 'pingtimestamp').orderBy('idx')\n",
    "duplicate_df2 = (duplicate_df2\n",
    "                 .withColumn('idx', row_number().over(windowSpec))\n",
    "                 .sort('trj_id', 'idx')\n",
    "                 .drop('idx')\n",
    "                )\n",
    "\n",
    "# Rename column\n",
    "for name in duplicate_df2.schema.names:\n",
    "    duplicate_df2 = duplicate_df2.withColumnRenamed(name, name + \"2\")\n",
    "\n",
    "# Add index\n",
    "duplicate_df2 = duplicate_df2.withColumn(\"idx2\", monotonically_increasing_id())\n",
    "windowSpec = Window.partitionBy('trj_id2').orderBy(\"idx2\")\n",
    "duplicate_df2 = duplicate_df2.withColumn(\"idx2\", row_number().over(windowSpec))\n",
    "\n",
    "# print_info(duplicate_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate_df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = duplicate_df.join(duplicate_df2, (duplicate_df.idx == duplicate_df2.idx2) & (duplicate_df.trj_id == duplicate_df2.trj_id2))\n",
    "final_df = (final_df\n",
    "            .filter(col(\"pingtimestamp\") < col(\"pingtimestamp2\"))\n",
    "            .select('rawlat', 'rawlng', 'pingtimestamp', 'rawlat2', 'rawlng2', 'pingtimestamp2')\n",
    "           )\n",
    "\n",
    "# print_info(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the data to BigQuery\n",
    "# final_df.write.format('bigquery') \\\n",
    "#     .option('table', 'jjkoh95:jjkoh.transform_grab_singapore_3_trips') \\\n",
    "#     .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'gs://hackathon.jjkoh.com/pipeline-dataproc-3'\n",
    "\n",
    "output_directory = OUTPUT_PATH\n",
    "output_files = output_directory + '/part-*'\n",
    "\n",
    "sql_context = SQLContext(sc)\n",
    "(final_df.write.option('header','true').format('csv').save(output_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://hackathon.jjkoh.com/traffic-cluster-density-20-kmeans.pkl\r\n",
      "gs://hackathon.jjkoh.com/traffic-cluster-speed-20-kmeans.pkl\r\n",
      "gs://hackathon.jjkoh.com/dataflow/\r\n",
      "gs://hackathon.jjkoh.com/pipeline-dataproc-2/\r\n",
      "gs://hackathon.jjkoh.com/pipeline-dataproc-3/\r\n",
      "gs://hackathon.jjkoh.com/pipeline-dataproc/\r\n",
      "gs://hackathon.jjkoh.com/pipeline/\r\n",
      "gs://hackathon.jjkoh.com/pipelinetrip/\r\n",
      "gs://hackathon.jjkoh.com/pipelinetrip1/\r\n",
      "gs://hackathon.jjkoh.com/raw-csv-jakarta/\r\n",
      "gs://hackathon.jjkoh.com/raw-csv-singapore/\r\n",
      "gs://hackathon.jjkoh.com/raw-parquet-jakarta/\r\n",
      "gs://hackathon.jjkoh.com/raw-parquet-singapore/\r\n",
      "gs://hackathon.jjkoh.com/tmp/\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://hackathon.jjkoh.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://hackathon.jjkoh.com/pipeline-dataproc-3/\r\n",
      "gs://hackathon.jjkoh.com/pipeline-dataproc-3/_SUCCESS\r\n",
      "gs://hackathon.jjkoh.com/pipeline-dataproc-3/part-00000-d4826f74-e12f-44ea-96bb-7e21e5a02d40-c000.csv\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://hackathon.jjkoh.com/pipeline-dataproc-3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}